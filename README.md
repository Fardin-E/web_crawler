## Web crawler made in Golang
# ğŸ•·ï¸ Web Crawler

A high-performance, concurrent web crawler built with Go, featuring configurable worker pools, intelligent URL deduplication, and Docker containerization.

## âœ¨ Features

- âš¡ **Concurrent Crawling** - Multi-threaded architecture with configurable worker pools
- ğŸ”„ **Smart URL Management** - Automatic deduplication and revisit control
- ğŸ¯ **Pattern Exclusion** - Filter unwanted domains (ads, trackers, etc.)
- ğŸ¤ **Politeness Delay** - Respects server resources with configurable delays
- ğŸ“¦ **Storage System** - Persistent file-based content storage
- ğŸ³ **Docker Ready** - Fully containerized with multi-stage builds
- ğŸ§ª **Well Tested** - Comprehensive test suite with 75% code coverage
- ğŸ”„ **CI/CD Pipeline** - Automated testing and building with GitHub Actions

## ğŸš€ Quick Start

### Using Docker (Recommended)

```bash
# Pull and run
docker run --rm web-crawler:latest crawl --url https://example.com --workers 5

# With persistent storage
docker run --rm -v $(pwd)/data:/home/crawler/data \
  web-crawler:latest crawl --url https://golang.org
```

### Using Go

```bash
# Clone the repository
git clone https://github.com/Fardin-E/web_crawler.git
cd web_crawler

# Install dependencies
go mod download

# Run the crawler
go run . crawl --url https://example.com --workers 10 --verbose
```

### Using Docker Compose

```bash
# Start crawler
docker-compose up crawler

# Or run in background
docker-compose up -d crawler
```

## ğŸ“– Usage

### Basic Crawling

```bash
# Crawl a single URL
./crawler crawl --url https://example.com

# Crawl with custom settings
./crawler crawl \
  --url https://example.com \
  --workers 20 \
  --output ./data \
  --verbose

# Crawl multiple URLs
./crawler crawl \
  --url https://site1.com \
  --url https://site2.com \
  --workers 10
```

### API Server Mode

```bash
# Start API server
./crawler serve --port 8080 --host 0.0.0.0

# Then access at http://localhost:8080
```

### Configuration Options

| Flag | Description | Default |
|------|-------------|---------|
| `--url` | URL(s) to crawl | Required |
| `--workers` | Number of concurrent workers | 5 |
| `--output` | Output directory | ./data |
| `--exclude` | Domains to exclude | None |
| `--verbose` | Enable verbose logging | false |
| `--port` | API server port (serve mode) | 8080 |

## ğŸ—ï¸ Architecture

### System Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Main Controller                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                         â”‚              â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚Frontierâ”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Workers  â”‚   â”‚Storage â”‚
â”‚        â”‚   URLs       â”‚  (Pool)   â”‚   â”‚        â”‚
â”‚ Queue  â”‚              â”‚           â”‚   â”‚ System â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                        â”‚  Parsers   â”‚
                        â”‚ (HTML/URL) â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

- **Frontier**: Manages URL queue with deduplication
- **Worker Pool**: Concurrent HTTP fetchers with rate limiting
- **Parser**: Extracts links and content from HTML
- **Storage**: Persists crawled content to disk
- **Processors**: Extensible pipeline for custom processing

### Worker Pool Pattern

```go
// Configurable number of workers
workers := 10

// Each worker processes URLs concurrently
for i := 0; i < workers; i++ {
    go worker.Start()
}

// Politeness delay per domain
time.Sleep(2 * time.Second)
```

## ğŸ§ª Testing

```bash
# Run all tests
go test ./...

# Run with coverage
go test -cover ./...

# Generate coverage report
go test -coverprofile=coverage.out ./...
go tool cover -html=coverage.out
```

### Test Coverage

| Package | Coverage | Tests |
|---------|----------|-------|
| `crawler` | 78% | 13 tests |
| `frontier` | 72% | 9 tests |
| **Total** | **75%** | **22 tests** |

## ğŸ³ Docker

### Build Image

```bash
# Build production image
docker build -t web-crawler:latest .

# Build without cache
docker build --no-cache -t web-crawler:latest .
```

### Image Details

- **Base Image**: Alpine Linux (minimal, secure)
- **Size**: ~15-20 MB (optimized with multi-stage build)
- **User**: Non-root user (security best practice)
- **Architecture**: Supports amd64 and arm64

### Multi-Stage Build

```dockerfile
# Stage 1: Build (golang:alpine)
- Download dependencies
- Run tests
- Compile binary

# Stage 2: Runtime (alpine:latest)
- Minimal image
- Copy binary only
- Non-root user
```

## ğŸ”„ CI/CD Pipeline

Automated pipeline runs on every commit:

```
Push to GitHub
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Run Tests    â”‚  â† go test -v ./crawler
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Build Binary â”‚  â† go build
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Build Docker  â”‚  â† docker build
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Verify Image  â”‚  â† docker run --help
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pipeline Time**: ~2 minutes âš¡

## ğŸ“Š Performance

### Benchmarks

| Metric | Value |
|--------|-------|
| Avg Response Time | ~250ms |
| URLs/second | ~40-50 (depends on workers) |
| Memory Usage | ~50-100 MB |
| CPU Usage | ~1-2 cores (10 workers) |

### Optimization Features

- âœ… Connection pooling
- âœ… Concurrent processing
- âœ… Efficient deduplication (map-based)
- âœ… Minimal memory footprint
- âœ… Rate limiting per domain

## ğŸ› ï¸ Development

### Project Structure

```
web_crawler/
â”œâ”€â”€ crawler/              # Core crawler logic
â”‚   â”œâ”€â”€ crawler.go       # Main crawler orchestration
â”‚   â”œâ”€â”€ worker.go        # Worker pool implementation
â”‚   â”œâ”€â”€ processor.go     # Content processors
â”‚   â”œâ”€â”€ queue.go         # URL queue management
â”‚   â””â”€â”€ *_test.go        # Test files
â”œâ”€â”€ frontier/            # URL frontier (queue + dedup)
â”‚   â”œâ”€â”€ frontier.go
â”‚   â””â”€â”€ frontier_test.go
â”œâ”€â”€ parser/              # HTML parsing & link extraction
â”‚   â””â”€â”€ parser.go
â”œâ”€â”€ storage/             # Content storage system
â”‚   â””â”€â”€ storage.go
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/       # CI/CD pipelines
â”‚       â””â”€â”€ ci-simple.yml
â”œâ”€â”€ Dockerfile           # Docker image definition
â”œâ”€â”€ docker-compose.yml   # Multi-container orchestration
â”œâ”€â”€ go.mod               # Go dependencies
â””â”€â”€ main.go              # CLI entry point
```

### Tech Stack

- **Language**: Go 1.24
- **CLI Framework**: Cobra
- **Testing**: Go testing framework
- **Containerization**: Docker
- **CI/CD**: GitHub Actions
- **Architecture**: Worker pool, channel-based concurrency

### Adding Custom Processors

```go
// Implement the Processor interface
type MyProcessor struct {}

func (p *MyProcessor) Process(result *CrawlResult) error {
    // Custom processing logic
    return nil
}

// Add to crawler
crawler.AddProcessor(&MyProcessor{})
```

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Code Quality

- Write tests for new features
- Maintain >70% code coverage
- Follow Go best practices
- Run `go fmt` before committing
- Ensure CI pipeline passes

## ğŸ“ Future Enhancements

- [ ] **Web Dashboard** - React-based UI for crawler management
- [ ] **Database Storage** - PostgreSQL/MongoDB support
- [ ] **Distributed Crawling** - Multi-node coordination
- [ ] **Robots.txt Support** - Respect crawl rules
- [ ] **Webhook Notifications** - Real-time crawl updates
- [ ] **Content Analysis** - NLP and sentiment analysis
- [ ] **Export Formats** - CSV, JSON, XML output
- [ ] **Metrics Dashboard** - Prometheus/Grafana integration

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Built with [Go](https://golang.org/)
- CLI powered by [Cobra](https://github.com/spf13/cobra)
- Inspired by best practices in web crawling

## ğŸ“§ Contact

**Fardin Rahman** - [@voidpntr](https://x.com/voidpntr) - fardinrahman647@gmail.com

Project Link: [https://github.com/Fardin-E/web_crawler](https://github.com/Fardin-E/web_crawler)

---

<div align="center">
  
**â­ Star this repository if you find it helpful!**

Made with â¤ï¸ and Go

</div>